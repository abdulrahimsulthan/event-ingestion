# Event Ingestion â€” Learning Progression

This document tracks how my skills grew as I moved from â€œI know Node.js basicsâ€ 
to â€œI understand ingestion systems, overload behavior, backpressure, and database limitsâ€.

The purpose is not motivation. It is evidence of engineering growth.

---

## ðŸŸ¢ Level 0 â€” Naive Understanding

### ðŸ§  Mental Model (Before)
- â€œEvents come in â†’ store in DB â†’ doneâ€
- Assumed Express + `express.json()` was fine for ingestion
- Thought more concurrency = more throughput
- Did not understand:
  - inflight limits
  - backpressure
  - DB latency as bottleneck
  - memory growth behavior
  - streaming vs buffering

### ðŸ§ª Experiments at this stage
- Built first `/ingest` endpoint using Express
- Parsed body using JSON middleware
- Inserted each request directly into Postgres
- Load-tested with `autocannon`

### ðŸš¨ What broke / what I learned
- Large payloads blew memory
- Whole-body buffering blocked event loop
- Postgres write latency capped throughput
- Concurrency increase did NOT increase RPS
- Realized:
  - requests are not free
  - DB I/O dominates performance
  - Node isnâ€™t the bottleneck, *the system is*

### ðŸ§© Key Concepts Unlocked
- Inflight request caps
- Backpressure mindset
- Postgres cannot be treated like a queue
- Throughput != concurrency

**This is where the â€œtoy web-app mindsetâ€ died.**

---

## ðŸŸ¡ Level 1 â€” First Principles Engineering

### ðŸ§  Mental Model (Now)
- Request handling must be **memory-bounded**
- DB writes must be **controlled**
- System should **fail fast when overloaded**
- Ingestion is NOT CRUD

### ðŸ§ª Experiments at this stage
- Removed `express.json()` to avoid buffering
- Switched to raw streaming `req.on('data')`
- Added:
  - MAX_SIZE payload guard
  - inflight connection cap
  - cleanup + close handling
- Stress-tested with:
  - 1MB / 5MB / 20MB payloads
  - 10â€“20000 concurrency
- Added metrics for:
  - RSS memory
  - heap usage
  - event loop lag

### ðŸ“Š Breakthrough Observations
- Memory spikes follow payload size
- RSS stays high due to fragmentation (not leaks)
- GC reduces heap but not RSS
- Throughput plateaued ~5â€“6k req/sec
  â†’ because Postgres was the bottleneck
- Inflight cap prevented catastrophic collapse
- Large single events = pathological clients

### ðŸ§© Concepts Gained (Real Engineering Skills)
- Overload protection is a feature
- Queueing is necessary
- DB writes must be decoupled
- Batching > single-row inserts
- Load patterns matter more than benchmarks
- Ingestion â‰  web server

**This is where I stopped thinking like a framework user
and started thinking like a systems engineer.**

---

## ðŸŸ£ Level 2 â€” Queue + Batch Writer (CURRENT STAGE)

I will document:

- my queue design decisions
- batch size strategy
- flush timing
- rejection policy when queue is full
- how throughput changes
- how latency behaves under load
- what surprised me
- what broke and why
- what I now understand differently

---
